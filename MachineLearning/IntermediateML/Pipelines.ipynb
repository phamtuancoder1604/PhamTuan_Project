{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7867d28d-f90d-4595-9a7d-02f5b5fc242d",
   "metadata": {},
   "source": [
    "#### Pipelines\n",
    "Pipelines are a simple way to <span style=\"background-color: #FFFF00\">keep your data preprocessing and modeling code organized</span>.Pipeline <span style=\"background-color: #FFFF00\">bundles preprocessing and modeling steps</span>\n",
    "- Cleaner Code: Accounting for data at each step of preprocessing can get messy. With a pipeline, you <span style=\"background-color: #FFFF00\">won't need to manually keep track of your training and validation data</span> at each step.\n",
    "- Fewer Bugs: There are <span style=\"background-color: #FFFF00\">fewer opportunities to misapply a step or forget a preprocessing step</span>.\n",
    "- Easier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.\n",
    "- More Options for Model Validation: You will see an example in the next tutorial, which <span style=\"background-color: #FFFF00\">covers cross-validation</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a68f3bf-0d1a-4b4a-8ea8-1b381a9236ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Read the data\n",
    "X_full = pd.read_csv('train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('test.csv', index_col='Id')\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "# Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924723cf-9119-4e55-9388-cdfc822d7246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols  = [col for col in X_train_full.columns if X_train_full[col].dtype == 'object' and \n",
    "                    X_train_full[col].nunique() < 10]\n",
    "# Select numerical columns\n",
    "numerical_cols = [col for col in X_train_full.columns if X_train_full[col].dtype in ['int64','float64']]\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248aa66-3121-4ab9-8aa8-bbf7aaf9b6eb",
   "metadata": {},
   "source": [
    "Step 1: Define Preprocessing Steps\n",
    "\n",
    "- Similar to how a pipeline bundles together preprocessing and modeling steps, we use the ColumnTransformer class to bundle together different preprocessing steps. The code below:\n",
    "\n",
    "<span style=\"background-color: #FFFF00\">imputes missing values in numerical data</span>, and\n",
    "imputes missing values and applies a <span style=\"background-color: #FFFF00\">one-hot encoding to categorical data</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26f03f2d-c29e-415b-8131-f01ede9564c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# Preprocessing for numerical data\n",
    "num_pre = SimpleImputer(strategy = 'mean')\n",
    "# Preprocessing for categorical data\n",
    "cat_pre = Pipeline(steps = [('imputer',SimpleImputer(strategy = 'most_frequent')),\n",
    "                            ('onehot',OneHotEncoder(handle_unknown = 'ignore'))])\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessing = ColumnTransformer(transformers = [('num',num_pre,numerical_cols),\n",
    "                                              ('cat',cat_pre,categorical_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a300a51-5c08-4335-8cd2-f370099a1d9b",
   "metadata": {},
   "source": [
    "Step 2: Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc8c37ce-c3cf-4886-8a01-6883478bf36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "model = RandomForestRegressor(n_estimators = 100 , random_state  = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f14d5-f532-4990-bf78-ed2982d2689d",
   "metadata": {},
   "source": [
    "Step 3: Create and Evaluate the PipelineÂ¶\n",
    "Finally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "749b4e9e-7584-4d9f-9794-c543fb1629a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mae is 17612.84342465753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error \n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps = [('preprocessor',preprocessing),\n",
    "                                  ('model',model)])\n",
    "# Preprocessing of training data, fit model\n",
    "clf.fit(X_train,y_train)\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = clf.predict(X_valid)\n",
    "# Evaluate the model\n",
    "print(f\"Mae is {mean_absolute_error(preds,y_valid)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
